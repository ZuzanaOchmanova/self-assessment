export type PartKey =
  | "dataCapture"
  | "storInteg"
  | "analyReport"
  | "govAuto";

export type StageCopy = {
  overview: string;
  quick: string;
  longterm: string;
};

export type PartStageCopyMap = {
  [part in PartKey]: {
    [stage in 0 | 1 | 2 | 3 | 4 | 5 | 6]: StageCopy;
  };
};

export const PART_STAGE_COPY: PartStageCopyMap = {
  dataCapture: {
    0: { overview: "Your data is still stuck in manual exports and one-off spreadsheets, leading to stale information, hidden errors, and hours wasted on repetitive tasks—making reliable insights nearly impossible. ", quick: "Automate just one core data pull (for example, your CRM) to run nightly into a shared folder or simple database using a basic scheduled script or free connector—so your team stops working from outdated files.", longterm: "Build a simple automated pipeline that regularly ingests all key systems, checks for missing or malformed data, and sends you alerts if something fails. Store everything in a central, searchable location, so everyone always works from the same, trusted source." },
    1: { overview: "At this level, you’ve automated at least one data pull—perhaps via a scheduled script—but most results still land in spreadsheets or slide decks without robust quality checks. This reduces some manual effort but leaves gaps: data can be outdated, inconsistencies persist, and analysts still spend time firefighting.", quick: "Enhance your existing script or connector by adding automated check—such as verifying row counts or schema fields—so failures surface immediately instead of sneaking into reports. ", longterm: "Create a simple, repeatable pipeline template that onboards new sources by plugging in source details rather than rewriting code. Pair it with nightly data-quality alerts and a shared folder or lightweight database to centralize everything reliably." },
    2: { overview: "You’ve graduated to reliable, automated data loads—multiple sources land in a central database each day without manual intervention. However, onboarding still requires custom work, data-quality checks are uneven, and failures may slip through, leading to occasional gaps or delays in your analytics.", quick: "Turn one of your custom load scripts into a reusable “template”—parameterize the source connection and table names so you can onboard new systems by editing a few settings instead of writing new code.", longterm: "Implement a simple pipeline manager to sequence and monitor all your data loads, attach automatic quality tests (row counts, null checks) after each step, and alert you on any failures—ensuring end-to-end reliability as you scale." },
    3: { overview: "At this stage, most of your key systems feed data automatically into a central database daily, eliminating manual exports—but bringing on new sources still requires custom scripts, and failures can slip through undetected. ", quick: "Pick one of your automated loads and add a simple daily health check—have your script verify row counts or data freshness and email you if it falls outside expected ranges.", longterm: "Set up a lightweight scheduler to manage all your data loads, attach automated quality tests at each step, and alert you on any failures—ensuring reliable, scalable pipelines as you grow. " },
    4: { overview: "You’ve automated most data loads and built reusable templates—but updates still occur in batches, and quality checks or alerts aren’t consistently enforced in real time. As a result, you can generally trust your data, but occasional delays or unnoticed failures can slow decision-making. ", quick: "Add real-time freshness checks: have your nightly pipelines emit a simple “heartbeat” signal to a dashboard or chat channel so you immediately see when data stops flowing. ", longterm: "Move to continuous ingestion with automated quality gates and alerting: deploy streaming or micro-batch processes that load new records as they arrive, run on-the-fly validation, and trigger notifications or fallbacks the moment any check fails—ensuring you never miss a beat. " },
    5: { overview: "You’ve built robust, automated pipelines that land data in near-real time with built-in quality checks, and most sources flow without manual steps. However, you may still rely on batch windows for certain feeds, and real-time alerts or SLA dashboards aren’t fully in place—so issues can go unnoticed until they impact reports.", quick: "Enable automated notifications for your existing pipelines: configure your scheduler or job runner to send a message or email alert whenever any data load runs late or fails—so you’re immediately aware of hiccups without checking dashboards manually. ", longterm: "Transition to a fully event-driven ingestion model: stream key data changes into your warehouse, hook every pipeline into a unified alerting framework with severity levels and escalation policies, and build SLA dashboards that track end-to-end latency and error rates in real time." },
    6: { overview: "You’ve achieved a truly state-of-the-art ingestion layer: data streams in continuously, schema evolutions are handled automatically, and real-time quality gates powered by machine-learning models flag anomalies before they reach downstream systems. This foundation not only fuels your analytics but also supplies clean, feature-ready data for advanced AI applications. ", quick: "Add a lightweight anomaly-detection check to one of your critical pipelines—use an opensource library or managed service to learn normal data patterns and alert on deviations automatically.", longterm: "Build out a full feature-store pipeline: automate extraction of key predictive features from your raw streams, version them in a centralized feature store, and expose them via APIs so your data science and application teams can deploy and monitor ML models with trusted, high-quality inputs." },
  },
  storInteg: {
    0: { overview: "Your data lives only in personal desktops or ad-hoc file shares, with no central storage, onboarding processes, schema management, documentation, or deduplication. This setup leads to version chaos, lost data, and immense manual effort whenever anyone needs to combine or query historical records—making trustworthy reporting virtually impossible.", quick: "Create a single shared folder or lightweight database and move one key dataset into it. Even copying yesterday’s exports into that central location gives your team a common “single source” to work from.", longterm: "Stand up a simple, central data repository and automate scheduled loads for all core systems. Layer on basic schema checks and deduplication logic so data is stored cleanly, consistently, and discoverably—transforming you from siloed files to a reliable, scalable data foundation." },
    1: { overview: "At this level, you’ve centralized some data into shared network drives or cloud folders—but there’s no true database or automated loading. Files sit in folders, and teams still copy, paste, or manually refresh spreadsheets, leading to version confusion and inconsistent data views. ", quick: "Pick one key dataset and load it into a simple, shared database table. Then schedule a nightly CSV import—so colleagues can query the same source instead of juggling multiple files. ", longterm: "Upgrade to a managed cloud database. Automate daily imports from all your core systems into that database, apply basic checks to catch errors, and use clear naming conventions—so your data is stored consistently, safely, and ready for reliable analysis." },
    2: { overview: "You’ve moved beyond shared folders into a central database or “data lake,” but loading new sources still takes custom work, and missing or duplicate records can slip through—so your dashboards aren’t fully reliable and require manual fixes. ", quick: "Turn one of your one-off load scripts into a reusable template: set it up so you only need to swap in the source name and credentials to onboard a new system, then run it on a schedule. This will save hours each time you add a data feed.", longterm: "Automate all your core data imports with a simple scheduler that runs your templated scripts, applies basic checks for missing or duplicate data, and writes clean, consistent tables to your database—so your BI dashboards always reflect up-to-date, trustworthy information." },
    3: { overview: "You’ve moved your core data into a central database or data lake with scheduled imports, so your team no longer chases scattered files—but adding new sources still takes custom scripts, and issues like missing or duplicate records can slip through before you notice.", quick: "Add a simple daily health check to your existing load jobs: have each job verify that it pulled the expected number of rows and flag you by email or chat if something is off.", longterm: "Set up a lightweight job manager to run all your import scripts in order, automatically retry on failure, apply basic sanity checks (e.g. no empty columns, no duplicates), and route clean data into your warehouse—so you get reliable, always-up-to-date tables without manual intervention." },
    4: { overview: "You’ve automated most batch loads and built reusable templates, but updates still happen on a schedule rather than continuously—and your team lacks consistent checks or alerts when something breaks. While your data is generally reliable, delays or unnoticed errors can still disrupt dashboards and downstream analytics. ", quick: "Add a simple “heartbeat” for each pipeline: after every load, write a timestamp to a dedicated “health” table or send a ping to email. That way you immediately spot if a process skips or stalls. ", longterm: "Move to near-real-time data flows and governance: introduce streaming or frequent microbatches for key sources, enforce basic schema and duplication checks on the fly, and set up automatic notifications for any failure—so your central repository stays fresh and trustworthy without manual oversight." },
    5: { overview: "You’ve automated all core batch loads into your warehouse, use reusable templates for new sources, and run daily health checks—but failures still only surface after the fact, and you lack a unified alerting system to catch issues before they impact reports. ", quick: "Hook your existing data loads into an alert channel: configure each job to send a notification on success and a clear error message on failure, so you immediately know when something goes wrong.", longterm: "Move to a fully monitored, SLA-driven ingestion framework: centralize all load jobs under a scheduler that enforces retries, captures detailed lineage, and routes alerts to the right teams with escalation policies—ensuring any data hiccup is caught and resolved before downstream dashboards ever see it." },
    6: { overview: "Your data infrastructure is rock-solid: you ingest all sources continuously, schema changes and duplicates are handled automatically, and every table is cataloged with full lineage. This level of rigor not only supports reliable analytics but also delivers clean, feature-ready data for advanced AI and machine-learning projects. ", quick: "Add a simple anomaly detector to one of your key data streams—use an open-source library or managed service to learn normal loads and send an alert when data patterns drift, reinforcing your real-time quality controls. ", longterm: "Build a centralized feature-store pipeline: automate the extraction, transformation, and versioning of machine-learning features from your real-time streams, and expose them via secure APIs so data science and application teams can deploy, monitor, and retrain models with confidence. " },
  },
  analyReport: {
    0: { overview: "Your reporting relies entirely on manual exports: every slide deck or spreadsheet is built from scratch for each request, with no automated reports or dashboards. This creates heavy workloads, inconsistent numbers, and long delays—making it impossible to scale reliable insights.", quick: "Automate one key report by scheduling a daily or weekly export. For example, set up your spreadsheet or BI tool to email the latest sales numbers every morning so stakeholders get up-to-date data without asking. ", longterm: "Adopt a simple dashboard platform connected to your shared data source. Build live dashboards that refresh automatically and allow basic filtering—so reports update themselves and everyone sees the same, current insights." },
    1: { overview: "You’ve automated basic report distribution—perhaps by emailing static spreadsheets or slide decks on a schedule—but interactivity and self-service are still missing. Stakeholders receive regular numbers, yet any new question requires manual tweaks, and inconsistencies can slip through without versioned definitions or traceable updates. ", quick: "Turn one of those emailed files into a basic dashboard view: connect your report to a simple BI tool and publish it read-only. Stakeholders can then see the latest numbers without opening spreadsheets, and you eliminate one manual export step. ", longterm: "Migrate your core reports into a true self-service analytics platform where KPIs live in a governed glossary, dashboards refresh automatically, and users can apply filters or date ranges on their own—freeing analysts from one-off requests and ensuring everyone always uses the same, version-controlled metrics." },
    2: { overview: "You’ve moved beyond static, scheduled reports and now publish auto-refreshing dashboards, but interactivity is limited and your KPI definitions may live in multiple places. Users can view up-to-date charts, yet any deeper slicing or new metric requires analyst intervention—and metrics may drift without clear version control. ", quick: "Pick one dashboard and add filter controls—allowing stakeholders to adjust date ranges or regions themselves. This small change reduces simple one-off requests and gives users faster, hands-on access to the data they need. ", longterm: "Centralize your KPIs in a single, governed glossary and integrate them into your BI tool—so every dashboard references the same, versioned definitions. Combine this with a selfservice analytics portal where users can explore governed datasets on their own, eliminating manual queries and ensuring consistency across all reports. " },
    3: { overview: "You’ve moved beyond scheduled static reports and now publish live dashboards that update automatically, but users still can’t slice data themselves, and key metrics live in multiple spreadsheets—leading to inconsistent figures and ongoing reliance on analysts for simple changes.", quick: "Enable basic filters on one dashboard—add dropdowns for date ranges or categories so stakeholders can adjust views without asking for new exports.", longterm: "Build a single “metric library” that lives in your BI tool: define each KPI once with a clear name, formula, and owner, then point all dashboards at this library. Combine that with a self-service portal where users can explore approved datasets, drill into charts, and export their own insights—freeing analysts from routine requests and ensuring everyone sees the same, accurate metrics. " },
    4: { overview: "You’ve built interactive, self-refreshing dashboards and centralized your KPIs, and business users can slice data without calling an analyst. However, you still lack automated alerting on usage drops or data anomalies, and some reports may rely on hard-coded metrics that aren’t version-controlled—so insights are fast but not yet fully trustable or self-governing.", quick: "Add a simple usage alert: configure your BI platform to notify you (via email or chat) when key dashboards see a sudden drop in views or when critical KPI values fall outside expected ranges, so you can investigate before stakeholders notice.", longterm: "Implement a governed reporting framework: move all KPI definitions into a central metric library with enforced version control, embed automated anomaly detection into your dashboards, and set up multi-channel alerts with escalation rules—so you deliver reliable, real-time insights with minimal manual oversight." },
    5: { overview: "You’ve built live dashboards with governed KPIs and self-service filters, and most reports refresh automatically—but you still lack proactive notifications for drops in usage or unexpected data shifts. As a result, issues can go unnoticed until stakeholders notice inconsistencies or stale numbers. ", quick: "Configure your BI platform to send a simple alert—via email or chat—whenever a key dashboard's view count falls sharply or a critical KPI deviates from its expected range. This gives you immediate visibility into potential problems. ", longterm: "Implement a full alerting framework: integrate your dashboards with a monitoring service that tracks usage trends, automatically detects anomalies in your data, and routes multichannel notifications with escalation rules—so you catch and resolve issues before they impact decision-making." },
    6: { overview: "You’ve built a fully self-service analytics environment with governed KPIs, parameterized dashboards, real-time alerts, and detailed engagement tracking. Business users not only explore data independently but also receive AI-powered insights and anomaly notifications—freeing analysts to focus on high-value modeling and strategy. ", quick: "Embed a simple predictive widget into one dashboard—use a pre-trained model (for example, time-series forecast or classification) to project next-period sales or flag likely churn, so users see “what’s next” alongside “what happened.” ", longterm: "Develop a fully integrated analytics platform where predictive models, anomaly detectors, and recommendation engines run continuously on live data feeds. Expose these via your dashboards and APIs—so every report not only shows the latest metrics but also delivers forward-looking insights, prescriptive actions, and end-to-end explanatory context. " },
  },
  govAuto: {
    0: { overview: "Your data environment has no formal ownership, access controls, monitoring, alerts, or documented policies. Anyone can access any data (or none), issues go unnoticed until users complain, and governance relies entirely on tribal knowledge—creating security risks, compliance gaps, and unpredictable data quality. ", quick: "Assign clear ownership for one critical dataset: pick a key table (e.g. customer records), name an “owner” responsible for its quality and access, and announce that assignment to your team—so there’s at least one person accountable.", longterm: "Implement basic access controls and monitoring: use your database or BI tool’s built-in role-based permissions to restrict who can view or edit sensitive data, set up automated health checks or audit logs to track usage, and document your initial policies in a shared, versioned guide—laying the groundwork for enterprise-grade governance and proactive alerting." },
    1: { overview: "You’ve taken the first step by listing data owners and policies in spreadsheets or shared docs, and perhaps set up basic email alerts—but nothing is enforced automatically. While roles and responsibilities are documented, assignments can become outdated and issues may slip through without active reminders or controls. ", quick: "Pick one critical dataset (e.g. customer records), confirm its owner in your spreadsheet, and set a recurring calendar reminder for that person to review its quality and access permissions each month. This simple cadence ensures ownership stays current without new tools.", longterm: "Move your owner and policy lists into a lightweight governance tool or catalog (even a simple wiki with template fields) that requires an owner’s name before a new dataset is created. Pair it with automated email or Slack reminders for pending reviews, so stewardship, access controls, and policy updates happen reliably and transparently. " },
    2: { overview: "You’ve moved beyond spot checks to running periodic, automated audits of data quality, lineage, and compliance—and you generate reports on the results. However, issues are only caught after the fact, ownership reminders aren’t automatic, and there’s no real-timealerting or auto-fix capability—so problems can linger until your next audit.", quick: "Link your scheduled audit jobs to notifications: configure each audit to send a summary to a shared team channel or email group. That way, your team sees failures as soon as they occur rather than discovering them in a periodic report. ", longterm: "Deploy a continuous monitoring framework that enforces quality and compliance rules in real time, tracks data lineage for every change, and automatically remediates or escalates on failures. Combine this with enforced stewardship—so every dataset update triggers ownership reminders and on-demand lineage lookups—giving you proactive, end-to-end governance. " },
    3: { overview: "You’ve moved beyond basic spreadsheets and spot checks: automated audits run on a schedule to validate data quality, track lineage, and check compliance. However, these checks only happen after the fact, and ownership reminders or real-time alerts aren’t yet in place—so issues can still linger until your next audit cycle. ", quick: "Configure your audit jobs to post their results immediately to a shared team channel or email group. That way, your team sees failures as soon as they occur, rather than discovering them in a periodic report. ", longterm: "Deploy a continuous monitoring framework that enforces your quality and compliance rules in real time, tracks lineage for every data change, and automatically notifies or remediates issues. Pair this with enforced stewardship—so dataset owners receive automated review reminders and can trace any data item back to its source at the click of a button. " },
    4: { overview: "You’ve moved beyond periodic audits and spreadsheet-based governance: access roles are enforced, ownership is tracked in a catalog, and scheduled checks validate data quality and lineage. However, alerts still only fire for critical failures, and policy updates or review reminders aren’t fully automated—so some issues may linger until stepped-up attention.", quick: "Configure your monitoring jobs to send real-time alerts to a dedicated teams channel or Teams group—not just email—for any rule failures or unauthorized access attempts. This ensures the right people see problems immediately. ", longterm: "Implement dynamic, attribute-based access controls and continuous remediation: integrate your governance tool with your data platform so policies (e.g., “only finance can view salary data”) are enforced automatically, every data change is tracked with full lineage, and any breach triggers multi-channel escalation—with built-in workflows for owners to review and resolve incidents." },
    5: { overview: "You’ve put nearly every governance control in place: ownership is enforced, access policies are applied automatically, continuous quality and compliance checks run in real time, and critical failures generate alerts. However, escalation paths and automated remediation aren’t yet fully wired—so some issues still require manual intervention before they’re resolved. ", quick: "Configure your alerting system to include escalation rules: have any unacknowledged critical alert automatically notify a backup owner or paging service after a short delay, ensuring nothing slips through the cracks. ", longterm: "Adopt a policy-as-code framework: codify every governance rule (ownership, access, quality, lineage) into version-controlled scripts or configurations that deploy automatically with each data pipeline change. Pair this with automated remediation workflows—so any policy breach triggers a corrective action (quarantine, rollback, or repair) without human hands. " },
    6: { overview: "Your governance is world-class: every dataset has an enforced owner, dynamic access policies adjust automatically, continuous quality and compliance checks run in real time, and any anomaly or breach triggers multi-channel alerts and automated remediation. You operate with full audit trails and policy-as-code, ensuring both security and agility at scale.", quick: "Add a “self-service remediation” button in your monitoring dashboard: when an alert fires, authorized owners can trigger a predefined corrective workflow (e.g. quarantine bad records or rerun a fix script) directly from the alert, speeding recovery. ", longterm: "Evolve into a fully autonomous governance layer by embedding your policy engine into every data pipeline: translate governance rules into code that runs pre- and post-ingest, automatically detect and resolve issues without human intervention, and surface only clean, compliant datasets for downstream analytics and AI—achieving true “set-and-forget” data governance. " },
  },
};
